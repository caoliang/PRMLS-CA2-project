{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "O0uj3KjFxlED",
    "outputId": "f0a41eb2-e9d9-49a1-d4b4-93af550a9040"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UGQ4pmHj0Br-"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn.metrics as metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,CSVLogger,LearningRateScheduler\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import AveragePooling2D\n",
    "from tensorflow.keras.layers import add\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LfKivrBP4VJJ"
   },
   "outputs": [],
   "source": [
    "#from google.colab import files\n",
    "#uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b2flHHXR08Tz"
   },
   "outputs": [],
   "source": [
    "def implt(img):\n",
    "    plt.figure()\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uqyqgB5U1CAV"
   },
   "outputs": [],
   "source": [
    "                            # Set up 'ggplot' style\n",
    "plt.style.use('ggplot')     # if want to use the default style, set 'classic'\n",
    "plt.rcParams['ytick.right']     = True\n",
    "plt.rcParams['ytick.labelright']= True\n",
    "plt.rcParams['ytick.left']      = False\n",
    "plt.rcParams['ytick.labelleft'] = False\n",
    "plt.rcParams['font.family']     = 'Arial'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "zBpLGESs1IN4",
    "outputId": "f40220c5-2179-41bd-d484-e24ed5f1244a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read X_train:  (2067, 128, 128, 3)\n",
      "Read y_train:  (2067,)\n",
      "Read X_test:  (1034, 128, 128, 3)\n",
      "Read y_test:  (1034,)\n"
     ]
    }
   ],
   "source": [
    "def read_data_set(h5_file='out.h5'):\n",
    "    with h5py.File(h5_file, 'r') as hf:\n",
    "        X_train = hf['X_train'].value\n",
    "        print('Read X_train: ', X_train.shape)        \n",
    "        \n",
    "        y_train = hf['y_train'].value\n",
    "        print('Read y_train: ', y_train.shape)        \n",
    "        \n",
    "        X_test = hf['X_test'].value\n",
    "        print('Read X_test: ', X_test.shape)        \n",
    "        \n",
    "        y_test = hf['y_test'].value\n",
    "        print('Read y_test: ', y_test.shape)        \n",
    "    \n",
    "    return (X_train, y_train, X_test, y_test)\n",
    "\n",
    "X_train_data, y_train_data, X_test_data, y_test_data = read_data_set(h5_file='/content/drive/My Drive/Colab/ca2data.h5' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3lxQRw1k1P7u"
   },
   "outputs": [],
   "source": [
    "#data            = cifar10.load_data()\n",
    "(trDat, trLbl)  = X_train_data, y_train_data\n",
    "(tsDat, tsLbl)  = X_test_data, y_test_data\n",
    "\n",
    "\n",
    "                            # Convert the data into 'float32'\n",
    "                            # Rescale the values from 0~255 to 0~1\n",
    "trDat       = trDat.astype('float32')/255\n",
    "tsDat       = tsDat.astype('float32')/255\n",
    "\n",
    "\n",
    "                            # Retrieve the row size of each image\n",
    "                            # Retrieve the column size of each image\n",
    "imgrows     = trDat.shape[1]\n",
    "imgclms     = trDat.shape[2]\n",
    "channel     = trDat.shape[3]\n",
    "\n",
    "\n",
    "                            # Perform one hot encoding on the labels\n",
    "                            # Retrieve the number of classes in this problem\n",
    "trLbl       = to_categorical(trLbl)\n",
    "tsLbl       = to_categorical(tsLbl)\n",
    "num_classes = tsLbl.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "QQxrC8I31avd",
    "outputId": "6f707471-3ecb-4a1b-f587-b753d19f5d95"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 128, 128, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Input_conv (Conv2D)             (None, 128, 128, 16) 448         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Input_bn (BatchNormalization)   (None, 128, 128, 16) 64          Input_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "Input_relu (Activation)         (None, 128, 128, 16) 0           Input_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 128, 128, 16) 0           Input_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk1_Res1_conv (Conv2D)    (None, 128, 128, 16) 2320        dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk1_Res1_bn (BatchNormali (None, 128, 128, 16) 64          Stg1_Blk1_Res1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk1_Res1_relu (Activation (None, 128, 128, 16) 0           Stg1_Blk1_Res1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk1_Res2_conv (Conv2D)    (None, 128, 128, 16) 2320        Stg1_Blk1_Res1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk1_Res2_bn (BatchNormali (None, 128, 128, 16) 64          Stg1_Blk1_Res2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk1_add (Add)             (None, 128, 128, 16) 0           dropout[0][0]                    \n",
      "                                                                 Stg1_Blk1_Res2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk1_relu (Activation)     (None, 128, 128, 16) 0           Stg1_Blk1_add[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk2_Res1_conv (Conv2D)    (None, 128, 128, 16) 2320        Stg1_Blk1_relu[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk2_Res1_bn (BatchNormali (None, 128, 128, 16) 64          Stg1_Blk2_Res1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk2_Res1_relu (Activation (None, 128, 128, 16) 0           Stg1_Blk2_Res1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk2_Res2_conv (Conv2D)    (None, 128, 128, 16) 2320        Stg1_Blk2_Res1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk2_Res2_bn (BatchNormali (None, 128, 128, 16) 64          Stg1_Blk2_Res2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk2_add (Add)             (None, 128, 128, 16) 0           Stg1_Blk1_relu[0][0]             \n",
      "                                                                 Stg1_Blk2_Res2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk2_relu (Activation)     (None, 128, 128, 16) 0           Stg1_Blk2_add[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk3_Res1_conv (Conv2D)    (None, 128, 128, 16) 2320        Stg1_Blk2_relu[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk3_Res1_bn (BatchNormali (None, 128, 128, 16) 64          Stg1_Blk3_Res1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk3_Res1_relu (Activation (None, 128, 128, 16) 0           Stg1_Blk3_Res1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk3_Res2_conv (Conv2D)    (None, 128, 128, 16) 2320        Stg1_Blk3_Res1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk3_Res2_bn (BatchNormali (None, 128, 128, 16) 64          Stg1_Blk3_Res2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk3_add (Add)             (None, 128, 128, 16) 0           Stg1_Blk2_relu[0][0]             \n",
      "                                                                 Stg1_Blk3_Res2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk3_relu (Activation)     (None, 128, 128, 16) 0           Stg1_Blk3_add[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk4_Res1_conv (Conv2D)    (None, 128, 128, 16) 2320        Stg1_Blk3_relu[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk4_Res1_bn (BatchNormali (None, 128, 128, 16) 64          Stg1_Blk4_Res1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk4_Res1_relu (Activation (None, 128, 128, 16) 0           Stg1_Blk4_Res1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk4_Res2_conv (Conv2D)    (None, 128, 128, 16) 2320        Stg1_Blk4_Res1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk4_Res2_bn (BatchNormali (None, 128, 128, 16) 64          Stg1_Blk4_Res2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk4_add (Add)             (None, 128, 128, 16) 0           Stg1_Blk3_relu[0][0]             \n",
      "                                                                 Stg1_Blk4_Res2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk4_relu (Activation)     (None, 128, 128, 16) 0           Stg1_Blk4_add[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk5_Res1_conv (Conv2D)    (None, 128, 128, 16) 2320        Stg1_Blk4_relu[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk5_Res1_bn (BatchNormali (None, 128, 128, 16) 64          Stg1_Blk5_Res1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk5_Res1_relu (Activation (None, 128, 128, 16) 0           Stg1_Blk5_Res1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk5_Res2_conv (Conv2D)    (None, 128, 128, 16) 2320        Stg1_Blk5_Res1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk5_Res2_bn (BatchNormali (None, 128, 128, 16) 64          Stg1_Blk5_Res2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk5_add (Add)             (None, 128, 128, 16) 0           Stg1_Blk4_relu[0][0]             \n",
      "                                                                 Stg1_Blk5_Res2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk5_relu (Activation)     (None, 128, 128, 16) 0           Stg1_Blk5_add[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk6_Res1_conv (Conv2D)    (None, 128, 128, 16) 2320        Stg1_Blk5_relu[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk6_Res1_bn (BatchNormali (None, 128, 128, 16) 64          Stg1_Blk6_Res1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk6_Res1_relu (Activation (None, 128, 128, 16) 0           Stg1_Blk6_Res1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk6_Res2_conv (Conv2D)    (None, 128, 128, 16) 2320        Stg1_Blk6_Res1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk6_Res2_bn (BatchNormali (None, 128, 128, 16) 64          Stg1_Blk6_Res2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk6_add (Add)             (None, 128, 128, 16) 0           Stg1_Blk5_relu[0][0]             \n",
      "                                                                 Stg1_Blk6_Res2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk6_relu (Activation)     (None, 128, 128, 16) 0           Stg1_Blk6_add[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk7_Res1_conv (Conv2D)    (None, 128, 128, 16) 2320        Stg1_Blk6_relu[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk7_Res1_bn (BatchNormali (None, 128, 128, 16) 64          Stg1_Blk7_Res1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk7_Res1_relu (Activation (None, 128, 128, 16) 0           Stg1_Blk7_Res1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk7_Res2_conv (Conv2D)    (None, 128, 128, 16) 2320        Stg1_Blk7_Res1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk7_Res2_bn (BatchNormali (None, 128, 128, 16) 64          Stg1_Blk7_Res2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk7_add (Add)             (None, 128, 128, 16) 0           Stg1_Blk6_relu[0][0]             \n",
      "                                                                 Stg1_Blk7_Res2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk7_relu (Activation)     (None, 128, 128, 16) 0           Stg1_Blk7_add[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 128, 128, 16) 0           Stg1_Blk7_relu[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk1_Res1_conv (Conv2D)    (None, 64, 64, 32)   4640        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk1_Res1_bn (BatchNormali (None, 64, 64, 32)   128         Stg2_Blk1_Res1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk1_Res1_relu (Activation (None, 64, 64, 32)   0           Stg2_Blk1_Res1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk1_Res2_conv (Conv2D)    (None, 64, 64, 32)   9248        Stg2_Blk1_Res1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk1_lin_conv (Conv2D)     (None, 64, 64, 32)   544         dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk1_Res2_bn (BatchNormali (None, 64, 64, 32)   128         Stg2_Blk1_Res2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk1_add (Add)             (None, 64, 64, 32)   0           Stg2_Blk1_lin_conv[0][0]         \n",
      "                                                                 Stg2_Blk1_Res2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk1_relu (Activation)     (None, 64, 64, 32)   0           Stg2_Blk1_add[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk2_Res1_conv (Conv2D)    (None, 64, 64, 32)   9248        Stg2_Blk1_relu[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk2_Res1_bn (BatchNormali (None, 64, 64, 32)   128         Stg2_Blk2_Res1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk2_Res1_relu (Activation (None, 64, 64, 32)   0           Stg2_Blk2_Res1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk2_Res2_conv (Conv2D)    (None, 64, 64, 32)   9248        Stg2_Blk2_Res1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk2_Res2_bn (BatchNormali (None, 64, 64, 32)   128         Stg2_Blk2_Res2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk2_add (Add)             (None, 64, 64, 32)   0           Stg2_Blk1_relu[0][0]             \n",
      "                                                                 Stg2_Blk2_Res2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk2_relu (Activation)     (None, 64, 64, 32)   0           Stg2_Blk2_add[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk3_Res1_conv (Conv2D)    (None, 64, 64, 32)   9248        Stg2_Blk2_relu[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk3_Res1_bn (BatchNormali (None, 64, 64, 32)   128         Stg2_Blk3_Res1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk3_Res1_relu (Activation (None, 64, 64, 32)   0           Stg2_Blk3_Res1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk3_Res2_conv (Conv2D)    (None, 64, 64, 32)   9248        Stg2_Blk3_Res1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk3_Res2_bn (BatchNormali (None, 64, 64, 32)   128         Stg2_Blk3_Res2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk3_add (Add)             (None, 64, 64, 32)   0           Stg2_Blk2_relu[0][0]             \n",
      "                                                                 Stg2_Blk3_Res2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk3_relu (Activation)     (None, 64, 64, 32)   0           Stg2_Blk3_add[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk4_Res1_conv (Conv2D)    (None, 64, 64, 32)   9248        Stg2_Blk3_relu[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk4_Res1_bn (BatchNormali (None, 64, 64, 32)   128         Stg2_Blk4_Res1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk4_Res1_relu (Activation (None, 64, 64, 32)   0           Stg2_Blk4_Res1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk4_Res2_conv (Conv2D)    (None, 64, 64, 32)   9248        Stg2_Blk4_Res1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk4_Res2_bn (BatchNormali (None, 64, 64, 32)   128         Stg2_Blk4_Res2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk4_add (Add)             (None, 64, 64, 32)   0           Stg2_Blk3_relu[0][0]             \n",
      "                                                                 Stg2_Blk4_Res2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk4_relu (Activation)     (None, 64, 64, 32)   0           Stg2_Blk4_add[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk5_Res1_conv (Conv2D)    (None, 64, 64, 32)   9248        Stg2_Blk4_relu[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk5_Res1_bn (BatchNormali (None, 64, 64, 32)   128         Stg2_Blk5_Res1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk5_Res1_relu (Activation (None, 64, 64, 32)   0           Stg2_Blk5_Res1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk5_Res2_conv (Conv2D)    (None, 64, 64, 32)   9248        Stg2_Blk5_Res1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk5_Res2_bn (BatchNormali (None, 64, 64, 32)   128         Stg2_Blk5_Res2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk5_add (Add)             (None, 64, 64, 32)   0           Stg2_Blk4_relu[0][0]             \n",
      "                                                                 Stg2_Blk5_Res2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk5_relu (Activation)     (None, 64, 64, 32)   0           Stg2_Blk5_add[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk6_Res1_conv (Conv2D)    (None, 64, 64, 32)   9248        Stg2_Blk5_relu[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk6_Res1_bn (BatchNormali (None, 64, 64, 32)   128         Stg2_Blk6_Res1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk6_Res1_relu (Activation (None, 64, 64, 32)   0           Stg2_Blk6_Res1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk6_Res2_conv (Conv2D)    (None, 64, 64, 32)   9248        Stg2_Blk6_Res1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk6_Res2_bn (BatchNormali (None, 64, 64, 32)   128         Stg2_Blk6_Res2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk6_add (Add)             (None, 64, 64, 32)   0           Stg2_Blk5_relu[0][0]             \n",
      "                                                                 Stg2_Blk6_Res2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk6_relu (Activation)     (None, 64, 64, 32)   0           Stg2_Blk6_add[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk7_Res1_conv (Conv2D)    (None, 64, 64, 32)   9248        Stg2_Blk6_relu[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk7_Res1_bn (BatchNormali (None, 64, 64, 32)   128         Stg2_Blk7_Res1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk7_Res1_relu (Activation (None, 64, 64, 32)   0           Stg2_Blk7_Res1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk7_Res2_conv (Conv2D)    (None, 64, 64, 32)   9248        Stg2_Blk7_Res1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk7_Res2_bn (BatchNormali (None, 64, 64, 32)   128         Stg2_Blk7_Res2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk7_add (Add)             (None, 64, 64, 32)   0           Stg2_Blk6_relu[0][0]             \n",
      "                                                                 Stg2_Blk7_Res2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk7_relu (Activation)     (None, 64, 64, 32)   0           Stg2_Blk7_add[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 64, 64, 32)   0           Stg2_Blk7_relu[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk1_Res1_conv (Conv2D)    (None, 32, 32, 64)   18496       dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk1_Res1_bn (BatchNormali (None, 32, 32, 64)   256         Stg3_Blk1_Res1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk1_Res1_relu (Activation (None, 32, 32, 64)   0           Stg3_Blk1_Res1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk1_Res2_conv (Conv2D)    (None, 32, 32, 64)   36928       Stg3_Blk1_Res1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk1_lin_conv (Conv2D)     (None, 32, 32, 64)   2112        dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk1_Res2_bn (BatchNormali (None, 32, 32, 64)   256         Stg3_Blk1_Res2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk1_add (Add)             (None, 32, 32, 64)   0           Stg3_Blk1_lin_conv[0][0]         \n",
      "                                                                 Stg3_Blk1_Res2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk1_relu (Activation)     (None, 32, 32, 64)   0           Stg3_Blk1_add[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk2_Res1_conv (Conv2D)    (None, 32, 32, 64)   36928       Stg3_Blk1_relu[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk2_Res1_bn (BatchNormali (None, 32, 32, 64)   256         Stg3_Blk2_Res1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk2_Res1_relu (Activation (None, 32, 32, 64)   0           Stg3_Blk2_Res1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk2_Res2_conv (Conv2D)    (None, 32, 32, 64)   36928       Stg3_Blk2_Res1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk2_Res2_bn (BatchNormali (None, 32, 32, 64)   256         Stg3_Blk2_Res2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk2_add (Add)             (None, 32, 32, 64)   0           Stg3_Blk1_relu[0][0]             \n",
      "                                                                 Stg3_Blk2_Res2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk2_relu (Activation)     (None, 32, 32, 64)   0           Stg3_Blk2_add[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk3_Res1_conv (Conv2D)    (None, 32, 32, 64)   36928       Stg3_Blk2_relu[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk3_Res1_bn (BatchNormali (None, 32, 32, 64)   256         Stg3_Blk3_Res1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk3_Res1_relu (Activation (None, 32, 32, 64)   0           Stg3_Blk3_Res1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk3_Res2_conv (Conv2D)    (None, 32, 32, 64)   36928       Stg3_Blk3_Res1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk3_Res2_bn (BatchNormali (None, 32, 32, 64)   256         Stg3_Blk3_Res2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk3_add (Add)             (None, 32, 32, 64)   0           Stg3_Blk2_relu[0][0]             \n",
      "                                                                 Stg3_Blk3_Res2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk3_relu (Activation)     (None, 32, 32, 64)   0           Stg3_Blk3_add[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk4_Res1_conv (Conv2D)    (None, 32, 32, 64)   36928       Stg3_Blk3_relu[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk4_Res1_bn (BatchNormali (None, 32, 32, 64)   256         Stg3_Blk4_Res1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk4_Res1_relu (Activation (None, 32, 32, 64)   0           Stg3_Blk4_Res1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk4_Res2_conv (Conv2D)    (None, 32, 32, 64)   36928       Stg3_Blk4_Res1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk4_Res2_bn (BatchNormali (None, 32, 32, 64)   256         Stg3_Blk4_Res2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk4_add (Add)             (None, 32, 32, 64)   0           Stg3_Blk3_relu[0][0]             \n",
      "                                                                 Stg3_Blk4_Res2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk4_relu (Activation)     (None, 32, 32, 64)   0           Stg3_Blk4_add[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk5_Res1_conv (Conv2D)    (None, 32, 32, 64)   36928       Stg3_Blk4_relu[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk5_Res1_bn (BatchNormali (None, 32, 32, 64)   256         Stg3_Blk5_Res1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk5_Res1_relu (Activation (None, 32, 32, 64)   0           Stg3_Blk5_Res1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk5_Res2_conv (Conv2D)    (None, 32, 32, 64)   36928       Stg3_Blk5_Res1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk5_Res2_bn (BatchNormali (None, 32, 32, 64)   256         Stg3_Blk5_Res2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk5_add (Add)             (None, 32, 32, 64)   0           Stg3_Blk4_relu[0][0]             \n",
      "                                                                 Stg3_Blk5_Res2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk5_relu (Activation)     (None, 32, 32, 64)   0           Stg3_Blk5_add[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk6_Res1_conv (Conv2D)    (None, 32, 32, 64)   36928       Stg3_Blk5_relu[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk6_Res1_bn (BatchNormali (None, 32, 32, 64)   256         Stg3_Blk6_Res1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk6_Res1_relu (Activation (None, 32, 32, 64)   0           Stg3_Blk6_Res1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk6_Res2_conv (Conv2D)    (None, 32, 32, 64)   36928       Stg3_Blk6_Res1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk6_Res2_bn (BatchNormali (None, 32, 32, 64)   256         Stg3_Blk6_Res2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk6_add (Add)             (None, 32, 32, 64)   0           Stg3_Blk5_relu[0][0]             \n",
      "                                                                 Stg3_Blk6_Res2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk6_relu (Activation)     (None, 32, 32, 64)   0           Stg3_Blk6_add[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk7_Res1_conv (Conv2D)    (None, 32, 32, 64)   36928       Stg3_Blk6_relu[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk7_Res1_bn (BatchNormali (None, 32, 32, 64)   256         Stg3_Blk7_Res1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk7_Res1_relu (Activation (None, 32, 32, 64)   0           Stg3_Blk7_Res1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk7_Res2_conv (Conv2D)    (None, 32, 32, 64)   36928       Stg3_Blk7_Res1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk7_Res2_bn (BatchNormali (None, 32, 32, 64)   256         Stg3_Blk7_Res2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk7_add (Add)             (None, 32, 32, 64)   0           Stg3_Blk6_relu[0][0]             \n",
      "                                                                 Stg3_Blk7_Res2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk7_relu (Activation)     (None, 32, 32, 64)   0           Stg3_Blk7_add[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 32, 32, 64)   0           Stg3_Blk7_relu[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk1_Res1_conv (Conv2D)    (None, 16, 16, 128)  73856       dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk1_Res1_bn (BatchNormali (None, 16, 16, 128)  512         Stg4_Blk1_Res1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk1_Res1_relu (Activation (None, 16, 16, 128)  0           Stg4_Blk1_Res1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk1_Res2_conv (Conv2D)    (None, 16, 16, 128)  147584      Stg4_Blk1_Res1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk1_lin_conv (Conv2D)     (None, 16, 16, 128)  8320        dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk1_Res2_bn (BatchNormali (None, 16, 16, 128)  512         Stg4_Blk1_Res2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk1_add (Add)             (None, 16, 16, 128)  0           Stg4_Blk1_lin_conv[0][0]         \n",
      "                                                                 Stg4_Blk1_Res2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk1_relu (Activation)     (None, 16, 16, 128)  0           Stg4_Blk1_add[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk2_Res1_conv (Conv2D)    (None, 16, 16, 128)  147584      Stg4_Blk1_relu[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk2_Res1_bn (BatchNormali (None, 16, 16, 128)  512         Stg4_Blk2_Res1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk2_Res1_relu (Activation (None, 16, 16, 128)  0           Stg4_Blk2_Res1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk2_Res2_conv (Conv2D)    (None, 16, 16, 128)  147584      Stg4_Blk2_Res1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk2_Res2_bn (BatchNormali (None, 16, 16, 128)  512         Stg4_Blk2_Res2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk2_add (Add)             (None, 16, 16, 128)  0           Stg4_Blk1_relu[0][0]             \n",
      "                                                                 Stg4_Blk2_Res2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk2_relu (Activation)     (None, 16, 16, 128)  0           Stg4_Blk2_add[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk3_Res1_conv (Conv2D)    (None, 16, 16, 128)  147584      Stg4_Blk2_relu[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk3_Res1_bn (BatchNormali (None, 16, 16, 128)  512         Stg4_Blk3_Res1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk3_Res1_relu (Activation (None, 16, 16, 128)  0           Stg4_Blk3_Res1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk3_Res2_conv (Conv2D)    (None, 16, 16, 128)  147584      Stg4_Blk3_Res1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk3_Res2_bn (BatchNormali (None, 16, 16, 128)  512         Stg4_Blk3_Res2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk3_add (Add)             (None, 16, 16, 128)  0           Stg4_Blk2_relu[0][0]             \n",
      "                                                                 Stg4_Blk3_Res2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk3_relu (Activation)     (None, 16, 16, 128)  0           Stg4_Blk3_add[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk4_Res1_conv (Conv2D)    (None, 16, 16, 128)  147584      Stg4_Blk3_relu[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk4_Res1_bn (BatchNormali (None, 16, 16, 128)  512         Stg4_Blk4_Res1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk4_Res1_relu (Activation (None, 16, 16, 128)  0           Stg4_Blk4_Res1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk4_Res2_conv (Conv2D)    (None, 16, 16, 128)  147584      Stg4_Blk4_Res1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk4_Res2_bn (BatchNormali (None, 16, 16, 128)  512         Stg4_Blk4_Res2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk4_add (Add)             (None, 16, 16, 128)  0           Stg4_Blk3_relu[0][0]             \n",
      "                                                                 Stg4_Blk4_Res2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk4_relu (Activation)     (None, 16, 16, 128)  0           Stg4_Blk4_add[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk5_Res1_conv (Conv2D)    (None, 16, 16, 128)  147584      Stg4_Blk4_relu[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk5_Res1_bn (BatchNormali (None, 16, 16, 128)  512         Stg4_Blk5_Res1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk5_Res1_relu (Activation (None, 16, 16, 128)  0           Stg4_Blk5_Res1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk5_Res2_conv (Conv2D)    (None, 16, 16, 128)  147584      Stg4_Blk5_Res1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk5_Res2_bn (BatchNormali (None, 16, 16, 128)  512         Stg4_Blk5_Res2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk5_add (Add)             (None, 16, 16, 128)  0           Stg4_Blk4_relu[0][0]             \n",
      "                                                                 Stg4_Blk5_Res2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk5_relu (Activation)     (None, 16, 16, 128)  0           Stg4_Blk5_add[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk6_Res1_conv (Conv2D)    (None, 16, 16, 128)  147584      Stg4_Blk5_relu[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk6_Res1_bn (BatchNormali (None, 16, 16, 128)  512         Stg4_Blk6_Res1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk6_Res1_relu (Activation (None, 16, 16, 128)  0           Stg4_Blk6_Res1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk6_Res2_conv (Conv2D)    (None, 16, 16, 128)  147584      Stg4_Blk6_Res1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk6_Res2_bn (BatchNormali (None, 16, 16, 128)  512         Stg4_Blk6_Res2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk6_add (Add)             (None, 16, 16, 128)  0           Stg4_Blk5_relu[0][0]             \n",
      "                                                                 Stg4_Blk6_Res2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk6_relu (Activation)     (None, 16, 16, 128)  0           Stg4_Blk6_add[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk7_Res1_conv (Conv2D)    (None, 16, 16, 128)  147584      Stg4_Blk6_relu[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk7_Res1_bn (BatchNormali (None, 16, 16, 128)  512         Stg4_Blk7_Res1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk7_Res1_relu (Activation (None, 16, 16, 128)  0           Stg4_Blk7_Res1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk7_Res2_conv (Conv2D)    (None, 16, 16, 128)  147584      Stg4_Blk7_Res1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk7_Res2_bn (BatchNormali (None, 16, 16, 128)  512         Stg4_Blk7_Res2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk7_add (Add)             (None, 16, 16, 128)  0           Stg4_Blk6_relu[0][0]             \n",
      "                                                                 Stg4_Blk7_Res2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk7_relu (Activation)     (None, 16, 16, 128)  0           Stg4_Blk7_add[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 16, 16, 128)  0           Stg4_Blk7_relu[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "AvgPool (AveragePooling2D)      (None, 2, 2, 128)    0           dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 2, 2, 128)    0           AvgPool[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 512)          0           dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 3)            1539        flatten[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 2,674,819\n",
      "Trainable params: 2,668,067\n",
      "Non-trainable params: 6,752\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "                            # fix random seed for reproducibility\n",
    "seed        = 42\n",
    "np.random.seed(seed)\n",
    "\n",
    "optmz       = optimizers.Adam(lr=0.001)\n",
    "modelname   = 'PRMLS_CA2'\n",
    "                            # define the deep learning model\n",
    "\n",
    "def resLyr(inputs,\n",
    "           numFilters=16,\n",
    "           kernelSz=3,\n",
    "           strides=1,\n",
    "           activation='relu',\n",
    "           batchNorm=True,\n",
    "           convFirst=True,\n",
    "           lyrName=None):\n",
    "  convLyr = Conv2D(numFilters,\n",
    "                  kernel_size=kernelSz,\n",
    "                  strides=strides,\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=l2(1e-4),\n",
    "                  name=lyrName+'_conv' if lyrName else None)\n",
    "  x = inputs\n",
    "  if convFirst:\n",
    "    x = convLyr(x)\n",
    "    if batchNorm:\n",
    "      x = BatchNormalization(name=lyrName+'_bn' if lyrName else None)(x)\n",
    "    if activation is not None:\n",
    "      x = Activation(activation,name=lyrName+'_'+activation if lyrName else None)(x)\n",
    "  else:\n",
    "    if batchNorm:\n",
    "      x = BatchNormalization(name=lyrName+'_bn' if lyrName else None)(x)\n",
    "    if activation is not None:\n",
    "      x = Activation(activation,name=lyrName+'_'+activation if lyrName else None)(x)\n",
    "    x = convLyr(x)\n",
    "\n",
    "  return x\n",
    "\n",
    "\n",
    "def resBlkV1(inputs,\n",
    "             numFilters=16,\n",
    "             numBlocks=7,\n",
    "             downsampleOnFirst=True,\n",
    "             names=None):\n",
    "  x = inputs\n",
    "  for run in range(0,numBlocks):\n",
    "    strides = 1\n",
    "    blkStr = str(run+1)\n",
    "    if downsampleOnFirst and run == 0:\n",
    "      strides = 2\n",
    "    y = resLyr(inputs=x,\n",
    "              numFilters=numFilters,\n",
    "              strides=strides,\n",
    "              lyrName=names+'_Blk'+blkStr+'_Res1' if names else None)\n",
    "    y = resLyr(inputs=y,\n",
    "              numFilters=numFilters,\n",
    "              activation=None,\n",
    "              lyrName=names+'_Blk'+blkStr+'_Res2' if names else None)\n",
    "    if downsampleOnFirst and run == 0:\n",
    "      x = resLyr(inputs=x,\n",
    "                numFilters = numFilters,\n",
    "                kernelSz=1,\n",
    "                strides=strides,\n",
    "                activation=None,\n",
    "                batchNorm=False,\n",
    "                lyrName=names+'_Blk'+blkStr+'_lin' if names else None)\n",
    "    x = add([x,y],\n",
    "           name=names+'_Blk'+blkStr+'_add' if names else None)\n",
    "    x  = Activation('relu',\n",
    "                   name=names+'_Blk'+blkStr+'_relu' if names else None)(x)\n",
    "     \n",
    "  return x\n",
    "    \n",
    "\n",
    "def createResNetV1(inputShape=(128,128,3),\n",
    "                   numClasses=3):\n",
    "  inputs = Input(shape=inputShape)\n",
    "  v = resLyr(inputs,\n",
    "            lyrName='Input')\n",
    "  \n",
    "  v = Dropout(0.2)(v)\n",
    "  \n",
    "  v = resBlkV1(inputs=v,\n",
    "              numFilters=16,\n",
    "              numBlocks=7,\n",
    "              downsampleOnFirst=False,\n",
    "              names='Stg1')\n",
    "  \n",
    "  v = Dropout(0.2)(v)\n",
    "  \n",
    "  v = resBlkV1(inputs=v,\n",
    "              numFilters=32,\n",
    "              numBlocks=7,\n",
    "              downsampleOnFirst=True,\n",
    "              names='Stg2')\n",
    "  \n",
    "  v = Dropout(0.2)(v)\n",
    "  \n",
    "  v = resBlkV1(inputs=v,\n",
    "              numFilters=64,\n",
    "              numBlocks=7,\n",
    "              downsampleOnFirst=True,\n",
    "              names='Stg3')\n",
    "  \n",
    "  v = Dropout(0.2)(v)\n",
    "  \n",
    "  v = resBlkV1(inputs=v,\n",
    "              numFilters=128,\n",
    "              numBlocks=7,\n",
    "              downsampleOnFirst=True,\n",
    "              names='Stg4')\n",
    "  \n",
    "  v = Dropout(0.2)(v)\n",
    "  \n",
    "  v = AveragePooling2D(pool_size=8,\n",
    "                      name='AvgPool')(v)\n",
    "  \n",
    "  v = Dropout(0.2)(v)\n",
    "  \n",
    "  v = Flatten()(v)\n",
    "  outputs = Dense(numClasses,\n",
    "                 activation='softmax',\n",
    "                 kernel_initializer='he_normal')(v)\n",
    "  model = Model(inputs=inputs,outputs=outputs)\n",
    "  model.compile(loss='categorical_crossentropy',\n",
    "               optimizer=optmz,\n",
    "               metrics=['accuracy'])\n",
    "    \n",
    "  return model\n",
    "\n",
    "\n",
    "                                # Setup the models\n",
    "model       = createResNetV1()  # This is meant for training\n",
    "modelGo     = createResNetV1()  # This is used for final testing\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2eUPaN9V1nBT"
   },
   "outputs": [],
   "source": [
    "def lrSchedule(epoch):\n",
    "    lr  = 1e-3\n",
    "    \n",
    "    if epoch > 160:\n",
    "        lr  *= 0.5e-3\n",
    "        \n",
    "    elif epoch > 140:\n",
    "        lr  *= 1e-3\n",
    "        \n",
    "    elif epoch > 120:\n",
    "        lr  *= 1e-2\n",
    "        \n",
    "    elif epoch > 80:\n",
    "        lr  *= 1e-1\n",
    "        \n",
    "    print('Learning rate: ', lr)\n",
    "    \n",
    "    return lr\n",
    "\n",
    "LRScheduler     = LearningRateScheduler(lrSchedule)\n",
    "\n",
    "                            # Create checkpoint for the training\n",
    "                            # This checkpoint performs model saving when\n",
    "                            # an epoch gives highest testing accuracy\n",
    "filepath        = '/content/drive/My Drive/Colab/' + modelname + \".hdf5\"\n",
    "checkpoint      = ModelCheckpoint(filepath, \n",
    "                                  monitor='val_acc', \n",
    "                                  verbose=0, \n",
    "                                  save_best_only=True, \n",
    "                                  mode='max')\n",
    "\n",
    "                            # Log the epoch detail into csv\n",
    "csv_logger      = CSVLogger('/content/drive/My Drive/Colab/' + modelname +'.csv')\n",
    "callbacks_list  = [checkpoint,csv_logger,LRScheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "x4B-p7Tf2KLX",
    "outputId": "848190c0-2bb2-4c84-c8bd-0a16be740b15"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  0.001\n",
      "Epoch 1/200\n",
      "65/64 [==============================] - 100s 2s/step - loss: 3.5085 - acc: 0.3832 - val_loss: 156.6228 - val_acc: 0.3453\n",
      "Learning rate:  0.001\n",
      "Epoch 2/200\n",
      "65/64 [==============================] - 48s 743ms/step - loss: 2.1737 - acc: 0.4117 - val_loss: 2.7378 - val_acc: 0.3482\n",
      "Learning rate:  0.001\n",
      "Epoch 3/200\n",
      "65/64 [==============================] - 48s 744ms/step - loss: 2.0060 - acc: 0.4035 - val_loss: 2.2456 - val_acc: 0.3665\n",
      "Learning rate:  0.001\n",
      "Epoch 4/200\n",
      "65/64 [==============================] - 48s 738ms/step - loss: 1.8790 - acc: 0.4267 - val_loss: 2.6841 - val_acc: 0.3762\n",
      "Learning rate:  0.001\n",
      "Epoch 5/200\n",
      "65/64 [==============================] - 48s 739ms/step - loss: 1.8596 - acc: 0.4562 - val_loss: 1.8656 - val_acc: 0.4439\n",
      "Learning rate:  0.001\n",
      "Epoch 6/200\n",
      "65/64 [==============================] - 46s 714ms/step - loss: 1.7990 - acc: 0.4577 - val_loss: 2.1828 - val_acc: 0.3897\n",
      "Learning rate:  0.001\n",
      "Epoch 7/200\n",
      "65/64 [==============================] - 46s 713ms/step - loss: 1.7649 - acc: 0.4625 - val_loss: 1.9650 - val_acc: 0.3694\n",
      "Learning rate:  0.001\n",
      "Epoch 8/200\n",
      "65/64 [==============================] - 47s 716ms/step - loss: 1.7242 - acc: 0.4644 - val_loss: 13.8265 - val_acc: 0.3656\n",
      "Learning rate:  0.001\n",
      "Epoch 9/200\n",
      "65/64 [==============================] - 48s 740ms/step - loss: 1.7440 - acc: 0.4712 - val_loss: 4.8440 - val_acc: 0.4632\n",
      "Learning rate:  0.001\n",
      "Epoch 10/200\n",
      "65/64 [==============================] - 46s 713ms/step - loss: 1.6750 - acc: 0.4910 - val_loss: 1.7082 - val_acc: 0.4584\n",
      "Learning rate:  0.001\n",
      "Epoch 11/200\n",
      "65/64 [==============================] - 48s 739ms/step - loss: 1.6627 - acc: 0.4809 - val_loss: 1.6172 - val_acc: 0.5164\n",
      "Learning rate:  0.001\n",
      "Epoch 12/200\n",
      "65/64 [==============================] - 46s 714ms/step - loss: 1.6112 - acc: 0.4920 - val_loss: 2.1892 - val_acc: 0.4516\n",
      "Learning rate:  0.001\n",
      "Epoch 13/200\n",
      "65/64 [==============================] - 46s 714ms/step - loss: 1.5680 - acc: 0.5191 - val_loss: 1.6944 - val_acc: 0.4932\n",
      "Learning rate:  0.001\n",
      "Epoch 14/200\n",
      "65/64 [==============================] - 48s 745ms/step - loss: 1.5272 - acc: 0.5336 - val_loss: 1.4879 - val_acc: 0.5609\n",
      "Learning rate:  0.001\n",
      "Epoch 15/200\n",
      "65/64 [==============================] - 46s 712ms/step - loss: 1.5062 - acc: 0.5336 - val_loss: 1.8002 - val_acc: 0.5000\n",
      "Learning rate:  0.001\n",
      "Epoch 16/200\n",
      "65/64 [==============================] - 48s 739ms/step - loss: 1.4647 - acc: 0.5389 - val_loss: 1.4527 - val_acc: 0.5822\n",
      "Learning rate:  0.001\n",
      "Epoch 17/200\n",
      "65/64 [==============================] - 48s 743ms/step - loss: 1.4850 - acc: 0.5307 - val_loss: 1.4771 - val_acc: 0.5841\n",
      "Learning rate:  0.001\n",
      "Epoch 18/200\n",
      "65/64 [==============================] - 46s 712ms/step - loss: 1.4131 - acc: 0.5525 - val_loss: 1.4547 - val_acc: 0.5329\n",
      "Learning rate:  0.001\n",
      "Epoch 19/200\n",
      "65/64 [==============================] - 46s 713ms/step - loss: 1.4020 - acc: 0.5597 - val_loss: 2.2576 - val_acc: 0.4516\n",
      "Learning rate:  0.001\n",
      "Epoch 20/200\n",
      "65/64 [==============================] - 48s 740ms/step - loss: 1.3766 - acc: 0.5617 - val_loss: 1.3166 - val_acc: 0.5909\n",
      "Learning rate:  0.001\n",
      "Epoch 21/200\n",
      "65/64 [==============================] - 47s 716ms/step - loss: 1.3325 - acc: 0.5965 - val_loss: 1.4210 - val_acc: 0.5271\n",
      "Learning rate:  0.001\n",
      "Epoch 22/200\n",
      "65/64 [==============================] - 46s 713ms/step - loss: 1.3448 - acc: 0.5704 - val_loss: 1.3614 - val_acc: 0.5629\n",
      "Learning rate:  0.001\n",
      "Epoch 23/200\n",
      "65/64 [==============================] - 46s 712ms/step - loss: 1.2991 - acc: 0.6009 - val_loss: 1.3471 - val_acc: 0.5435\n",
      "Learning rate:  0.001\n",
      "Epoch 24/200\n",
      "65/64 [==============================] - 46s 713ms/step - loss: 1.2864 - acc: 0.5917 - val_loss: 1.3157 - val_acc: 0.5571\n",
      "Learning rate:  0.001\n",
      "Epoch 25/200\n",
      "65/64 [==============================] - 47s 716ms/step - loss: 1.2642 - acc: 0.5989 - val_loss: 1.3882 - val_acc: 0.5464\n",
      "Learning rate:  0.001\n",
      "Epoch 26/200\n",
      "65/64 [==============================] - 48s 736ms/step - loss: 1.2233 - acc: 0.6284 - val_loss: 1.2769 - val_acc: 0.5996\n",
      "Learning rate:  0.001\n",
      "Epoch 27/200\n",
      "65/64 [==============================] - 48s 743ms/step - loss: 1.2135 - acc: 0.6091 - val_loss: 1.1667 - val_acc: 0.6634\n",
      "Learning rate:  0.001\n",
      "Epoch 28/200\n",
      "65/64 [==============================] - 46s 714ms/step - loss: 1.1918 - acc: 0.6149 - val_loss: 1.2880 - val_acc: 0.5716\n",
      "Learning rate:  0.001\n",
      "Epoch 29/200\n",
      "65/64 [==============================] - 47s 715ms/step - loss: 1.1969 - acc: 0.6018 - val_loss: 1.2971 - val_acc: 0.5696\n",
      "Learning rate:  0.001\n",
      "Epoch 30/200\n",
      "65/64 [==============================] - 46s 715ms/step - loss: 1.1609 - acc: 0.6251 - val_loss: 1.3001 - val_acc: 0.5754\n",
      "Learning rate:  0.001\n",
      "Epoch 31/200\n",
      "65/64 [==============================] - 46s 715ms/step - loss: 1.1492 - acc: 0.6236 - val_loss: 1.3362 - val_acc: 0.5745\n",
      "Learning rate:  0.001\n",
      "Epoch 32/200\n",
      "65/64 [==============================] - 46s 712ms/step - loss: 1.1289 - acc: 0.6289 - val_loss: 1.3542 - val_acc: 0.6074\n",
      "Learning rate:  0.001\n",
      "Epoch 33/200\n",
      "65/64 [==============================] - 46s 715ms/step - loss: 1.1137 - acc: 0.6357 - val_loss: 1.6169 - val_acc: 0.5271\n",
      "Learning rate:  0.001\n",
      "Epoch 34/200\n",
      "65/64 [==============================] - 47s 717ms/step - loss: 1.0993 - acc: 0.6343 - val_loss: 1.9329 - val_acc: 0.4014\n",
      "Learning rate:  0.001\n",
      "Epoch 35/200\n",
      "65/64 [==============================] - 46s 715ms/step - loss: 1.0932 - acc: 0.6502 - val_loss: 1.5005 - val_acc: 0.4720\n",
      "Learning rate:  0.001\n",
      "Epoch 36/200\n",
      "65/64 [==============================] - 46s 710ms/step - loss: 1.0600 - acc: 0.6463 - val_loss: 1.2364 - val_acc: 0.5948\n",
      "Learning rate:  0.001\n",
      "Epoch 37/200\n",
      "65/64 [==============================] - 46s 714ms/step - loss: 1.0552 - acc: 0.6526 - val_loss: 1.8657 - val_acc: 0.4255\n",
      "Learning rate:  0.001\n",
      "Epoch 38/200\n",
      "65/64 [==============================] - 47s 717ms/step - loss: 1.0628 - acc: 0.6517 - val_loss: 1.1388 - val_acc: 0.6170\n",
      "Learning rate:  0.001\n",
      "Epoch 39/200\n",
      "65/64 [==============================] - 47s 716ms/step - loss: 1.0236 - acc: 0.6807 - val_loss: 1.2944 - val_acc: 0.6103\n",
      "Learning rate:  0.001\n",
      "Epoch 40/200\n",
      "65/64 [==============================] - 47s 724ms/step - loss: 1.0259 - acc: 0.6604 - val_loss: 1.1198 - val_acc: 0.6277\n",
      "Learning rate:  0.001\n",
      "Epoch 41/200\n",
      "65/64 [==============================] - 46s 714ms/step - loss: 1.0289 - acc: 0.6512 - val_loss: 1.1664 - val_acc: 0.5832\n",
      "Learning rate:  0.001\n",
      "Epoch 42/200\n",
      "65/64 [==============================] - 46s 712ms/step - loss: 1.0269 - acc: 0.6609 - val_loss: 1.1174 - val_acc: 0.6441\n",
      "Learning rate:  0.001\n",
      "Epoch 43/200\n",
      "65/64 [==============================] - 46s 703ms/step - loss: 0.9890 - acc: 0.6681 - val_loss: 1.0973 - val_acc: 0.6509\n",
      "Learning rate:  0.001\n",
      "Epoch 44/200\n",
      "65/64 [==============================] - 46s 707ms/step - loss: 0.9833 - acc: 0.6768 - val_loss: 1.8953 - val_acc: 0.6228\n",
      "Learning rate:  0.001\n",
      "Epoch 45/200\n",
      "65/64 [==============================] - 48s 741ms/step - loss: 0.9720 - acc: 0.6836 - val_loss: 1.0896 - val_acc: 0.6654\n",
      "Learning rate:  0.001\n",
      "Epoch 46/200\n",
      "65/64 [==============================] - 47s 716ms/step - loss: 0.9815 - acc: 0.6696 - val_loss: 1.4178 - val_acc: 0.4507\n",
      "Learning rate:  0.001\n",
      "Epoch 47/200\n",
      "65/64 [==============================] - 46s 714ms/step - loss: 0.9567 - acc: 0.6739 - val_loss: 1.1897 - val_acc: 0.6277\n",
      "Learning rate:  0.001\n",
      "Epoch 48/200\n",
      "65/64 [==============================] - 47s 716ms/step - loss: 0.9596 - acc: 0.6865 - val_loss: 1.0413 - val_acc: 0.6190\n",
      "Learning rate:  0.001\n",
      "Epoch 49/200\n",
      "65/64 [==============================] - 48s 745ms/step - loss: 0.9365 - acc: 0.6928 - val_loss: 1.0000 - val_acc: 0.6741\n",
      "Learning rate:  0.001\n",
      "Epoch 50/200\n",
      "65/64 [==============================] - 47s 716ms/step - loss: 0.9553 - acc: 0.6797 - val_loss: 1.5112 - val_acc: 0.5242\n",
      "Learning rate:  0.001\n",
      "Epoch 51/200\n",
      "65/64 [==============================] - 46s 713ms/step - loss: 0.9346 - acc: 0.6913 - val_loss: 1.1649 - val_acc: 0.6035\n",
      "Learning rate:  0.001\n",
      "Epoch 52/200\n",
      "65/64 [==============================] - 46s 715ms/step - loss: 0.9082 - acc: 0.7063 - val_loss: 1.0632 - val_acc: 0.6712\n",
      "Learning rate:  0.001\n",
      "Epoch 53/200\n",
      "65/64 [==============================] - 46s 715ms/step - loss: 0.9287 - acc: 0.6909 - val_loss: 1.0449 - val_acc: 0.6673\n",
      "Learning rate:  0.001\n",
      "Epoch 54/200\n",
      "65/64 [==============================] - 46s 713ms/step - loss: 0.9075 - acc: 0.6996 - val_loss: 1.8242 - val_acc: 0.4072\n",
      "Learning rate:  0.001\n",
      "Epoch 55/200\n",
      "65/64 [==============================] - 47s 717ms/step - loss: 0.9012 - acc: 0.6996 - val_loss: 1.1628 - val_acc: 0.5996\n",
      "Learning rate:  0.001\n",
      "Epoch 56/200\n",
      "65/64 [==============================] - 46s 715ms/step - loss: 0.9114 - acc: 0.6899 - val_loss: 1.2238 - val_acc: 0.5087\n",
      "Learning rate:  0.001\n",
      "Epoch 57/200\n",
      "65/64 [==============================] - 46s 715ms/step - loss: 0.8968 - acc: 0.7005 - val_loss: 2.1476 - val_acc: 0.6576\n",
      "Learning rate:  0.001\n",
      "Epoch 58/200\n",
      "65/64 [==============================] - 48s 741ms/step - loss: 0.8977 - acc: 0.6923 - val_loss: 0.9840 - val_acc: 0.6944\n",
      "Learning rate:  0.001\n",
      "Epoch 59/200\n",
      "65/64 [==============================] - 46s 714ms/step - loss: 0.8624 - acc: 0.7165 - val_loss: 1.3984 - val_acc: 0.4700\n",
      "Learning rate:  0.001\n",
      "Epoch 60/200\n",
      "65/64 [==============================] - 47s 716ms/step - loss: 0.8702 - acc: 0.7039 - val_loss: 1.0967 - val_acc: 0.6799\n",
      "Learning rate:  0.001\n",
      "Epoch 61/200\n",
      "65/64 [==============================] - 46s 713ms/step - loss: 0.8779 - acc: 0.7073 - val_loss: 1.3490 - val_acc: 0.5561\n",
      "Learning rate:  0.001\n",
      "Epoch 62/200\n",
      "65/64 [==============================] - 46s 713ms/step - loss: 0.8385 - acc: 0.7175 - val_loss: 1.0805 - val_acc: 0.5977\n",
      "Learning rate:  0.001\n",
      "Epoch 63/200\n",
      "65/64 [==============================] - 47s 722ms/step - loss: 0.8495 - acc: 0.7228 - val_loss: 0.9553 - val_acc: 0.6702\n",
      "Learning rate:  0.001\n",
      "Epoch 64/200\n",
      "65/64 [==============================] - 47s 717ms/step - loss: 0.8127 - acc: 0.7344 - val_loss: 1.1013 - val_acc: 0.6161\n",
      "Learning rate:  0.001\n",
      "Epoch 65/200\n",
      "65/64 [==============================] - 46s 715ms/step - loss: 0.8084 - acc: 0.7218 - val_loss: 1.1514 - val_acc: 0.6006\n",
      "Learning rate:  0.001\n",
      "Epoch 66/200\n",
      "65/64 [==============================] - 48s 742ms/step - loss: 0.8215 - acc: 0.7373 - val_loss: 0.8786 - val_acc: 0.7099\n",
      "Learning rate:  0.001\n",
      "Epoch 67/200\n",
      "65/64 [==============================] - 46s 712ms/step - loss: 0.8345 - acc: 0.7315 - val_loss: 1.0452 - val_acc: 0.7099\n",
      "Learning rate:  0.001\n",
      "Epoch 68/200\n",
      "65/64 [==============================] - 46s 713ms/step - loss: 0.8174 - acc: 0.7218 - val_loss: 1.0616 - val_acc: 0.6615\n",
      "Learning rate:  0.001\n",
      "Epoch 69/200\n",
      "65/64 [==============================] - 46s 712ms/step - loss: 0.7785 - acc: 0.7508 - val_loss: 1.4215 - val_acc: 0.5484\n",
      "Learning rate:  0.001\n",
      "Epoch 70/200\n",
      "65/64 [==============================] - 46s 713ms/step - loss: 0.8134 - acc: 0.7325 - val_loss: 1.3750 - val_acc: 0.5455\n",
      "Learning rate:  0.001\n",
      "Epoch 71/200\n",
      "65/64 [==============================] - 48s 738ms/step - loss: 0.7796 - acc: 0.7436 - val_loss: 0.9074 - val_acc: 0.7137\n",
      "Learning rate:  0.001\n",
      "Epoch 72/200\n",
      "65/64 [==============================] - 46s 710ms/step - loss: 0.8079 - acc: 0.7349 - val_loss: 0.8737 - val_acc: 0.7118\n",
      "Learning rate:  0.001\n",
      "Epoch 73/200\n",
      "65/64 [==============================] - 46s 710ms/step - loss: 0.7870 - acc: 0.7499 - val_loss: 1.1469 - val_acc: 0.6654\n",
      "Learning rate:  0.001\n",
      "Epoch 74/200\n",
      "65/64 [==============================] - 48s 742ms/step - loss: 0.7558 - acc: 0.7537 - val_loss: 0.9213 - val_acc: 0.7147\n",
      "Learning rate:  0.001\n",
      "Epoch 75/200\n",
      "65/64 [==============================] - 47s 716ms/step - loss: 0.7610 - acc: 0.7557 - val_loss: 1.1756 - val_acc: 0.6750\n",
      "Learning rate:  0.001\n",
      "Epoch 76/200\n",
      "65/64 [==============================] - 46s 714ms/step - loss: 0.7716 - acc: 0.7504 - val_loss: 1.0033 - val_acc: 0.6422\n",
      "Learning rate:  0.001\n",
      "Epoch 77/200\n",
      "65/64 [==============================] - 46s 713ms/step - loss: 0.7923 - acc: 0.7378 - val_loss: 0.9674 - val_acc: 0.6876\n",
      "Learning rate:  0.001\n",
      "Epoch 78/200\n",
      "65/64 [==============================] - 46s 714ms/step - loss: 0.7976 - acc: 0.7383 - val_loss: 1.0899 - val_acc: 0.5996\n",
      "Learning rate:  0.001\n",
      "Epoch 79/200\n",
      "65/64 [==============================] - 46s 712ms/step - loss: 0.7831 - acc: 0.7397 - val_loss: 2.2834 - val_acc: 0.4284\n",
      "Learning rate:  0.001\n",
      "Epoch 80/200\n",
      "65/64 [==============================] - 47s 715ms/step - loss: 0.7787 - acc: 0.7523 - val_loss: 2.7454 - val_acc: 0.6489\n",
      "Learning rate:  0.001\n",
      "Epoch 81/200\n",
      "65/64 [==============================] - 48s 740ms/step - loss: 0.7594 - acc: 0.7629 - val_loss: 0.9302 - val_acc: 0.7282\n",
      "Learning rate:  0.0001\n",
      "Epoch 82/200\n",
      "65/64 [==============================] - 48s 740ms/step - loss: 0.7014 - acc: 0.7789 - val_loss: 0.7693 - val_acc: 0.7679\n",
      "Learning rate:  0.0001\n",
      "Epoch 83/200\n",
      "65/64 [==============================] - 48s 738ms/step - loss: 0.6611 - acc: 0.7987 - val_loss: 0.7601 - val_acc: 0.7747\n",
      "Learning rate:  0.0001\n",
      "Epoch 84/200\n",
      "65/64 [==============================] - 47s 716ms/step - loss: 0.6360 - acc: 0.8104 - val_loss: 0.7906 - val_acc: 0.7640\n",
      "Learning rate:  0.0001\n",
      "Epoch 85/200\n",
      "65/64 [==============================] - 46s 712ms/step - loss: 0.6496 - acc: 0.8002 - val_loss: 0.7954 - val_acc: 0.7582\n",
      "Learning rate:  0.0001\n",
      "Epoch 86/200\n",
      "65/64 [==============================] - 46s 712ms/step - loss: 0.6288 - acc: 0.8012 - val_loss: 0.7927 - val_acc: 0.7747\n",
      "Learning rate:  0.0001\n",
      "Epoch 87/200\n",
      "65/64 [==============================] - 46s 715ms/step - loss: 0.6247 - acc: 0.8191 - val_loss: 0.8519 - val_acc: 0.7447\n",
      "Learning rate:  0.0001\n",
      "Epoch 88/200\n",
      "65/64 [==============================] - 46s 715ms/step - loss: 0.6053 - acc: 0.8157 - val_loss: 0.7884 - val_acc: 0.7698\n",
      "Learning rate:  0.0001\n",
      "Epoch 89/200\n",
      "65/64 [==============================] - 46s 714ms/step - loss: 0.6027 - acc: 0.8157 - val_loss: 0.7940 - val_acc: 0.7747\n",
      "Learning rate:  0.0001\n",
      "Epoch 90/200\n",
      "65/64 [==============================] - 48s 739ms/step - loss: 0.5885 - acc: 0.8278 - val_loss: 0.7853 - val_acc: 0.7766\n",
      "Learning rate:  0.0001\n",
      "Epoch 91/200\n",
      "65/64 [==============================] - 46s 711ms/step - loss: 0.5839 - acc: 0.8312 - val_loss: 0.8533 - val_acc: 0.7592\n",
      "Learning rate:  0.0001\n",
      "Epoch 92/200\n",
      "65/64 [==============================] - 47s 716ms/step - loss: 0.5934 - acc: 0.8341 - val_loss: 0.8725 - val_acc: 0.7495\n",
      "Learning rate:  0.0001\n",
      "Epoch 93/200\n",
      "65/64 [==============================] - 48s 738ms/step - loss: 0.5909 - acc: 0.8186 - val_loss: 0.7940 - val_acc: 0.7785\n",
      "Learning rate:  0.0001\n",
      "Epoch 94/200\n",
      "65/64 [==============================] - 46s 715ms/step - loss: 0.5906 - acc: 0.8220 - val_loss: 0.8035 - val_acc: 0.7747\n",
      "Learning rate:  0.0001\n",
      "Epoch 95/200\n",
      "65/64 [==============================] - 46s 712ms/step - loss: 0.5902 - acc: 0.8215 - val_loss: 0.8615 - val_acc: 0.7592\n",
      "Learning rate:  0.0001\n",
      "Epoch 96/200\n",
      "65/64 [==============================] - 46s 715ms/step - loss: 0.5663 - acc: 0.8389 - val_loss: 0.8415 - val_acc: 0.7698\n",
      "Learning rate:  0.0001\n",
      "Epoch 97/200\n",
      "65/64 [==============================] - 47s 716ms/step - loss: 0.5583 - acc: 0.8476 - val_loss: 0.8436 - val_acc: 0.7631\n",
      "Learning rate:  0.0001\n",
      "Epoch 98/200\n",
      "65/64 [==============================] - 46s 714ms/step - loss: 0.5802 - acc: 0.8350 - val_loss: 0.8307 - val_acc: 0.7718\n",
      "Learning rate:  0.0001\n",
      "Epoch 99/200\n",
      "65/64 [==============================] - 47s 716ms/step - loss: 0.5658 - acc: 0.8394 - val_loss: 0.8425 - val_acc: 0.7650\n",
      "Learning rate:  0.0001\n",
      "Epoch 100/200\n",
      "65/64 [==============================] - 46s 714ms/step - loss: 0.5354 - acc: 0.8447 - val_loss: 0.8617 - val_acc: 0.7621\n",
      "Learning rate:  0.0001\n",
      "Epoch 101/200\n",
      "65/64 [==============================] - 46s 715ms/step - loss: 0.5559 - acc: 0.8520 - val_loss: 0.9079 - val_acc: 0.7631\n",
      "Learning rate:  0.0001\n",
      "Epoch 102/200\n",
      "65/64 [==============================] - 46s 714ms/step - loss: 0.5597 - acc: 0.8428 - val_loss: 0.9270 - val_acc: 0.7447\n",
      "Learning rate:  0.0001\n",
      "Epoch 103/200\n",
      "65/64 [==============================] - 46s 713ms/step - loss: 0.5409 - acc: 0.8520 - val_loss: 0.8973 - val_acc: 0.7573\n",
      "Learning rate:  0.0001\n",
      "Epoch 104/200\n",
      "65/64 [==============================] - 46s 709ms/step - loss: 0.5507 - acc: 0.8360 - val_loss: 0.9009 - val_acc: 0.7495\n",
      "Learning rate:  0.0001\n",
      "Epoch 105/200\n",
      "65/64 [==============================] - 46s 712ms/step - loss: 0.5191 - acc: 0.8515 - val_loss: 0.8936 - val_acc: 0.7669\n",
      "Learning rate:  0.0001\n",
      "Epoch 106/200\n",
      "65/64 [==============================] - 46s 714ms/step - loss: 0.5457 - acc: 0.8413 - val_loss: 0.8683 - val_acc: 0.7515\n",
      "Learning rate:  0.0001\n",
      "Epoch 107/200\n",
      "65/64 [==============================] - 46s 714ms/step - loss: 0.5301 - acc: 0.8544 - val_loss: 0.8955 - val_acc: 0.7582\n",
      "Learning rate:  0.0001\n",
      "Epoch 108/200\n",
      "65/64 [==============================] - 46s 714ms/step - loss: 0.5354 - acc: 0.8447 - val_loss: 0.8985 - val_acc: 0.7505\n",
      "Learning rate:  0.0001\n",
      "Epoch 109/200\n",
      "65/64 [==============================] - 47s 723ms/step - loss: 0.5223 - acc: 0.8524 - val_loss: 0.8429 - val_acc: 0.7708\n",
      "Learning rate:  0.0001\n",
      "Epoch 110/200\n",
      "65/64 [==============================] - 48s 746ms/step - loss: 0.5034 - acc: 0.8602 - val_loss: 0.8266 - val_acc: 0.7795\n",
      "Learning rate:  0.0001\n",
      "Epoch 111/200\n",
      "65/64 [==============================] - 47s 716ms/step - loss: 0.5284 - acc: 0.8553 - val_loss: 0.8611 - val_acc: 0.7563\n",
      "Learning rate:  0.0001\n",
      "Epoch 112/200\n",
      "65/64 [==============================] - 46s 715ms/step - loss: 0.4973 - acc: 0.8679 - val_loss: 0.9481 - val_acc: 0.7379\n",
      "Learning rate:  0.0001\n",
      "Epoch 113/200\n",
      "65/64 [==============================] - 46s 714ms/step - loss: 0.5017 - acc: 0.8582 - val_loss: 0.8710 - val_acc: 0.7650\n",
      "Learning rate:  0.0001\n",
      "Epoch 114/200\n",
      "65/64 [==============================] - 46s 714ms/step - loss: 0.5103 - acc: 0.8660 - val_loss: 0.8945 - val_acc: 0.7437\n",
      "Learning rate:  0.0001\n",
      "Epoch 115/200\n",
      "65/64 [==============================] - 46s 712ms/step - loss: 0.4971 - acc: 0.8631 - val_loss: 1.0383 - val_acc: 0.7302\n",
      "Learning rate:  0.0001\n",
      "Epoch 116/200\n",
      "65/64 [==============================] - 46s 713ms/step - loss: 0.5000 - acc: 0.8694 - val_loss: 0.8303 - val_acc: 0.7669\n",
      "Learning rate:  0.0001\n",
      "Epoch 117/200\n",
      "65/64 [==============================] - 46s 712ms/step - loss: 0.5135 - acc: 0.8549 - val_loss: 0.8465 - val_acc: 0.7679\n",
      "Learning rate:  0.0001\n",
      "Epoch 118/200\n",
      "65/64 [==============================] - 46s 713ms/step - loss: 0.5056 - acc: 0.8616 - val_loss: 0.9530 - val_acc: 0.7427\n",
      "Learning rate:  0.0001\n",
      "Epoch 119/200\n",
      "65/64 [==============================] - 46s 711ms/step - loss: 0.4852 - acc: 0.8708 - val_loss: 0.9186 - val_acc: 0.7708\n",
      "Learning rate:  0.0001\n",
      "Epoch 120/200\n",
      "65/64 [==============================] - 46s 704ms/step - loss: 0.5035 - acc: 0.8607 - val_loss: 0.9461 - val_acc: 0.7418\n",
      "Learning rate:  0.0001\n",
      "Epoch 121/200\n",
      "65/64 [==============================] - 46s 707ms/step - loss: 0.4578 - acc: 0.8776 - val_loss: 0.9936 - val_acc: 0.7331\n",
      "Learning rate:  1e-05\n",
      "Epoch 122/200\n",
      "65/64 [==============================] - 46s 714ms/step - loss: 0.4537 - acc: 0.8926 - val_loss: 0.9732 - val_acc: 0.7389\n",
      "Learning rate:  1e-05\n",
      "Epoch 123/200\n",
      "65/64 [==============================] - 46s 713ms/step - loss: 0.4661 - acc: 0.8737 - val_loss: 0.9387 - val_acc: 0.7437\n",
      "Learning rate:  1e-05\n",
      "Epoch 124/200\n",
      "65/64 [==============================] - 46s 714ms/step - loss: 0.4490 - acc: 0.8791 - val_loss: 0.9193 - val_acc: 0.7534\n",
      "Learning rate:  1e-05\n",
      "Epoch 125/200\n",
      "65/64 [==============================] - 46s 715ms/step - loss: 0.4642 - acc: 0.8771 - val_loss: 0.9168 - val_acc: 0.7534\n",
      "Learning rate:  1e-05\n",
      "Epoch 126/200\n",
      "65/64 [==============================] - 46s 711ms/step - loss: 0.4340 - acc: 0.8911 - val_loss: 0.9028 - val_acc: 0.7582\n",
      "Learning rate:  1e-05\n",
      "Epoch 127/200\n",
      "65/64 [==============================] - 46s 711ms/step - loss: 0.4579 - acc: 0.8781 - val_loss: 0.9049 - val_acc: 0.7573\n",
      "Learning rate:  1e-05\n",
      "Epoch 128/200\n",
      "65/64 [==============================] - 47s 721ms/step - loss: 0.4451 - acc: 0.8878 - val_loss: 0.9124 - val_acc: 0.7631\n",
      "Learning rate:  1e-05\n",
      "Epoch 129/200\n",
      "65/64 [==============================] - 47s 716ms/step - loss: 0.4526 - acc: 0.8805 - val_loss: 0.9197 - val_acc: 0.7602\n",
      "Learning rate:  1e-05\n",
      "Epoch 130/200\n",
      "65/64 [==============================] - 46s 714ms/step - loss: 0.4540 - acc: 0.8907 - val_loss: 0.9318 - val_acc: 0.7592\n",
      "Learning rate:  1e-05\n",
      "Epoch 131/200\n",
      "65/64 [==============================] - 46s 713ms/step - loss: 0.4458 - acc: 0.8786 - val_loss: 0.9578 - val_acc: 0.7592\n",
      "Learning rate:  1e-05\n",
      "Epoch 132/200\n",
      "65/64 [==============================] - 46s 714ms/step - loss: 0.4466 - acc: 0.8878 - val_loss: 0.9438 - val_acc: 0.7602\n",
      "Learning rate:  1e-05\n",
      "Epoch 133/200\n",
      "65/64 [==============================] - 47s 717ms/step - loss: 0.4587 - acc: 0.8766 - val_loss: 0.9467 - val_acc: 0.7534\n",
      "Learning rate:  1e-05\n",
      "Epoch 134/200\n",
      "65/64 [==============================] - 46s 714ms/step - loss: 0.4212 - acc: 0.8979 - val_loss: 0.9408 - val_acc: 0.7621\n",
      "Learning rate:  1e-05\n",
      "Epoch 135/200\n",
      "65/64 [==============================] - 46s 713ms/step - loss: 0.4279 - acc: 0.8882 - val_loss: 0.9512 - val_acc: 0.7582\n",
      "Learning rate:  1e-05\n",
      "Epoch 136/200\n",
      "65/64 [==============================] - 46s 714ms/step - loss: 0.4306 - acc: 0.8907 - val_loss: 0.9508 - val_acc: 0.7621\n",
      "Learning rate:  1e-05\n",
      "Epoch 137/200\n",
      "65/64 [==============================] - 47s 716ms/step - loss: 0.4462 - acc: 0.8902 - val_loss: 0.9463 - val_acc: 0.7631\n",
      "Learning rate:  1e-05\n",
      "Epoch 138/200\n",
      "65/64 [==============================] - 46s 711ms/step - loss: 0.4443 - acc: 0.8834 - val_loss: 0.9678 - val_acc: 0.7544\n",
      "Learning rate:  1e-05\n",
      "Epoch 139/200\n",
      "65/64 [==============================] - 46s 715ms/step - loss: 0.4613 - acc: 0.8815 - val_loss: 0.9553 - val_acc: 0.7582\n",
      "Learning rate:  1e-05\n",
      "Epoch 140/200\n",
      "65/64 [==============================] - 47s 723ms/step - loss: 0.4310 - acc: 0.8868 - val_loss: 0.9413 - val_acc: 0.7611\n",
      "Learning rate:  1e-05\n",
      "Epoch 141/200\n",
      "65/64 [==============================] - 46s 712ms/step - loss: 0.4237 - acc: 0.8873 - val_loss: 0.9406 - val_acc: 0.7621\n",
      "Learning rate:  1e-06\n",
      "Epoch 142/200\n",
      "65/64 [==============================] - 46s 715ms/step - loss: 0.4212 - acc: 0.8887 - val_loss: 0.9431 - val_acc: 0.7592\n",
      "Learning rate:  1e-06\n",
      "Epoch 143/200\n",
      "65/64 [==============================] - 46s 711ms/step - loss: 0.4428 - acc: 0.8805 - val_loss: 0.9446 - val_acc: 0.7573\n",
      "Learning rate:  1e-06\n",
      "Epoch 144/200\n",
      "65/64 [==============================] - 46s 714ms/step - loss: 0.4257 - acc: 0.8945 - val_loss: 0.9448 - val_acc: 0.7582\n",
      "Learning rate:  1e-06\n",
      "Epoch 145/200\n",
      "65/64 [==============================] - 46s 715ms/step - loss: 0.4149 - acc: 0.9018 - val_loss: 0.9434 - val_acc: 0.7582\n",
      "Learning rate:  1e-06\n",
      "Epoch 146/200\n",
      "65/64 [==============================] - 46s 714ms/step - loss: 0.4313 - acc: 0.8878 - val_loss: 0.9502 - val_acc: 0.7573\n",
      "Learning rate:  1e-06\n",
      "Epoch 147/200\n",
      "65/64 [==============================] - 47s 717ms/step - loss: 0.4409 - acc: 0.8824 - val_loss: 0.9429 - val_acc: 0.7611\n",
      "Learning rate:  1e-06\n",
      "Epoch 148/200\n",
      "65/64 [==============================] - 47s 718ms/step - loss: 0.4256 - acc: 0.8936 - val_loss: 0.9420 - val_acc: 0.7602\n",
      "Learning rate:  1e-06\n",
      "Epoch 149/200\n",
      "65/64 [==============================] - 47s 715ms/step - loss: 0.4146 - acc: 0.8989 - val_loss: 0.9477 - val_acc: 0.7611\n",
      "Learning rate:  1e-06\n",
      "Epoch 150/200\n",
      "65/64 [==============================] - 46s 708ms/step - loss: 0.4380 - acc: 0.8868 - val_loss: 0.9565 - val_acc: 0.7611\n",
      "Learning rate:  1e-06\n",
      "Epoch 151/200\n",
      "65/64 [==============================] - 47s 717ms/step - loss: 0.4247 - acc: 0.8974 - val_loss: 0.9555 - val_acc: 0.7592\n",
      "Learning rate:  1e-06\n",
      "Epoch 152/200\n",
      "65/64 [==============================] - 47s 717ms/step - loss: 0.4370 - acc: 0.8902 - val_loss: 0.9606 - val_acc: 0.7592\n",
      "Learning rate:  1e-06\n",
      "Epoch 153/200\n",
      "65/64 [==============================] - 46s 714ms/step - loss: 0.4269 - acc: 0.8921 - val_loss: 0.9623 - val_acc: 0.7582\n",
      "Learning rate:  1e-06\n",
      "Epoch 154/200\n",
      "65/64 [==============================] - 46s 714ms/step - loss: 0.4520 - acc: 0.8844 - val_loss: 0.9591 - val_acc: 0.7592\n",
      "Learning rate:  1e-06\n",
      "Epoch 155/200\n",
      "65/64 [==============================] - 47s 718ms/step - loss: 0.4200 - acc: 0.8960 - val_loss: 0.9635 - val_acc: 0.7582\n",
      "Learning rate:  1e-06\n",
      "Epoch 156/200\n",
      "65/64 [==============================] - 46s 715ms/step - loss: 0.4355 - acc: 0.8892 - val_loss: 0.9627 - val_acc: 0.7582\n",
      "Learning rate:  1e-06\n",
      "Epoch 157/200\n",
      "65/64 [==============================] - 47s 718ms/step - loss: 0.4514 - acc: 0.8839 - val_loss: 0.9666 - val_acc: 0.7573\n",
      "Learning rate:  1e-06\n",
      "Epoch 158/200\n",
      "65/64 [==============================] - 46s 713ms/step - loss: 0.4433 - acc: 0.8897 - val_loss: 0.9602 - val_acc: 0.7621\n",
      "Learning rate:  1e-06\n",
      "Epoch 159/200\n",
      "65/64 [==============================] - 46s 708ms/step - loss: 0.4224 - acc: 0.8936 - val_loss: 0.9566 - val_acc: 0.7611\n",
      "Learning rate:  1e-06\n",
      "Epoch 160/200\n",
      "65/64 [==============================] - 46s 712ms/step - loss: 0.4268 - acc: 0.8878 - val_loss: 0.9583 - val_acc: 0.7592\n",
      "Learning rate:  1e-06\n",
      "Epoch 161/200\n",
      "65/64 [==============================] - 47s 717ms/step - loss: 0.4135 - acc: 0.8989 - val_loss: 0.9603 - val_acc: 0.7611\n",
      "Learning rate:  5e-07\n",
      "Epoch 162/200\n",
      "65/64 [==============================] - 47s 717ms/step - loss: 0.4147 - acc: 0.8970 - val_loss: 0.9654 - val_acc: 0.7573\n",
      "Learning rate:  5e-07\n",
      "Epoch 163/200\n",
      "65/64 [==============================] - 47s 718ms/step - loss: 0.4344 - acc: 0.8849 - val_loss: 0.9657 - val_acc: 0.7582\n",
      "Learning rate:  5e-07\n",
      "Epoch 164/200\n",
      "65/64 [==============================] - 46s 715ms/step - loss: 0.4094 - acc: 0.8936 - val_loss: 0.9646 - val_acc: 0.7553\n",
      "Learning rate:  5e-07\n",
      "Epoch 165/200\n",
      "65/64 [==============================] - 46s 715ms/step - loss: 0.4447 - acc: 0.8858 - val_loss: 0.9616 - val_acc: 0.7592\n",
      "Learning rate:  5e-07\n",
      "Epoch 166/200\n",
      "65/64 [==============================] - 46s 712ms/step - loss: 0.4214 - acc: 0.8936 - val_loss: 0.9600 - val_acc: 0.7611\n",
      "Learning rate:  5e-07\n",
      "Epoch 167/200\n",
      "65/64 [==============================] - 47s 716ms/step - loss: 0.4177 - acc: 0.8931 - val_loss: 0.9578 - val_acc: 0.7631\n",
      "Learning rate:  5e-07\n",
      "Epoch 168/200\n",
      "65/64 [==============================] - 47s 719ms/step - loss: 0.4310 - acc: 0.8931 - val_loss: 0.9572 - val_acc: 0.7592\n",
      "Learning rate:  5e-07\n",
      "Epoch 169/200\n",
      "65/64 [==============================] - 47s 717ms/step - loss: 0.4272 - acc: 0.8950 - val_loss: 0.9621 - val_acc: 0.7592\n",
      "Learning rate:  5e-07\n",
      "Epoch 170/200\n",
      "65/64 [==============================] - 46s 713ms/step - loss: 0.4123 - acc: 0.8989 - val_loss: 0.9654 - val_acc: 0.7611\n",
      "Learning rate:  5e-07\n",
      "Epoch 171/200\n",
      "65/64 [==============================] - 46s 712ms/step - loss: 0.4150 - acc: 0.8994 - val_loss: 0.9624 - val_acc: 0.7611\n",
      "Learning rate:  5e-07\n",
      "Epoch 172/200\n",
      "65/64 [==============================] - 47s 717ms/step - loss: 0.4134 - acc: 0.8979 - val_loss: 0.9670 - val_acc: 0.7573\n",
      "Learning rate:  5e-07\n",
      "Epoch 173/200\n",
      "65/64 [==============================] - 46s 714ms/step - loss: 0.4263 - acc: 0.8873 - val_loss: 0.9610 - val_acc: 0.7602\n",
      "Learning rate:  5e-07\n",
      "Epoch 174/200\n",
      "65/64 [==============================] - 46s 715ms/step - loss: 0.4322 - acc: 0.8873 - val_loss: 0.9630 - val_acc: 0.7592\n",
      "Learning rate:  5e-07\n",
      "Epoch 175/200\n",
      "65/64 [==============================] - 47s 715ms/step - loss: 0.4224 - acc: 0.8970 - val_loss: 0.9670 - val_acc: 0.7582\n",
      "Learning rate:  5e-07\n",
      "Epoch 176/200\n",
      "65/64 [==============================] - 46s 715ms/step - loss: 0.4100 - acc: 0.9008 - val_loss: 0.9676 - val_acc: 0.7563\n",
      "Learning rate:  5e-07\n",
      "Epoch 177/200\n",
      "65/64 [==============================] - 46s 713ms/step - loss: 0.4296 - acc: 0.8945 - val_loss: 0.9743 - val_acc: 0.7573\n",
      "Learning rate:  5e-07\n",
      "Epoch 178/200\n",
      "65/64 [==============================] - 47s 716ms/step - loss: 0.4220 - acc: 0.8892 - val_loss: 0.9718 - val_acc: 0.7582\n",
      "Learning rate:  5e-07\n",
      "Epoch 179/200\n",
      "65/64 [==============================] - 46s 715ms/step - loss: 0.4362 - acc: 0.8863 - val_loss: 0.9608 - val_acc: 0.7621\n",
      "Learning rate:  5e-07\n",
      "Epoch 180/200\n",
      "65/64 [==============================] - 47s 716ms/step - loss: 0.4298 - acc: 0.8882 - val_loss: 0.9649 - val_acc: 0.7602\n",
      "Learning rate:  5e-07\n",
      "Epoch 181/200\n",
      "65/64 [==============================] - 47s 718ms/step - loss: 0.4275 - acc: 0.8936 - val_loss: 0.9609 - val_acc: 0.7621\n",
      "Learning rate:  5e-07\n",
      "Epoch 182/200\n",
      "65/64 [==============================] - 46s 710ms/step - loss: 0.4413 - acc: 0.8902 - val_loss: 0.9632 - val_acc: 0.7611\n",
      "Learning rate:  5e-07\n",
      "Epoch 183/200\n",
      "65/64 [==============================] - 46s 712ms/step - loss: 0.4248 - acc: 0.8960 - val_loss: 0.9563 - val_acc: 0.7592\n",
      "Learning rate:  5e-07\n",
      "Epoch 184/200\n",
      "65/64 [==============================] - 46s 713ms/step - loss: 0.4443 - acc: 0.8776 - val_loss: 0.9563 - val_acc: 0.7631\n",
      "Learning rate:  5e-07\n",
      "Epoch 185/200\n",
      "65/64 [==============================] - 47s 718ms/step - loss: 0.4363 - acc: 0.8873 - val_loss: 0.9617 - val_acc: 0.7621\n",
      "Learning rate:  5e-07\n",
      "Epoch 186/200\n",
      "65/64 [==============================] - 47s 716ms/step - loss: 0.4380 - acc: 0.8786 - val_loss: 0.9610 - val_acc: 0.7631\n",
      "Learning rate:  5e-07\n",
      "Epoch 187/200\n",
      "65/64 [==============================] - 46s 712ms/step - loss: 0.4354 - acc: 0.8795 - val_loss: 0.9643 - val_acc: 0.7582\n",
      "Learning rate:  5e-07\n",
      "Epoch 188/200\n",
      "65/64 [==============================] - 46s 712ms/step - loss: 0.4337 - acc: 0.8882 - val_loss: 0.9669 - val_acc: 0.7602\n",
      "Learning rate:  5e-07\n",
      "Epoch 189/200\n",
      "65/64 [==============================] - 46s 713ms/step - loss: 0.4207 - acc: 0.8907 - val_loss: 0.9625 - val_acc: 0.7592\n",
      "Learning rate:  5e-07\n",
      "Epoch 190/200\n",
      "65/64 [==============================] - 46s 711ms/step - loss: 0.4200 - acc: 0.8907 - val_loss: 0.9686 - val_acc: 0.7602\n",
      "Learning rate:  5e-07\n",
      "Epoch 191/200\n",
      "65/64 [==============================] - 46s 712ms/step - loss: 0.4281 - acc: 0.8853 - val_loss: 0.9642 - val_acc: 0.7592\n",
      "Learning rate:  5e-07\n",
      "Epoch 192/200\n",
      "65/64 [==============================] - 46s 715ms/step - loss: 0.4084 - acc: 0.8999 - val_loss: 0.9714 - val_acc: 0.7602\n",
      "Learning rate:  5e-07\n",
      "Epoch 193/200\n",
      "65/64 [==============================] - 46s 714ms/step - loss: 0.4449 - acc: 0.8878 - val_loss: 0.9641 - val_acc: 0.7602\n",
      "Learning rate:  5e-07\n",
      "Epoch 194/200\n",
      "65/64 [==============================] - 46s 713ms/step - loss: 0.4256 - acc: 0.8916 - val_loss: 0.9729 - val_acc: 0.7544\n",
      "Learning rate:  5e-07\n",
      "Epoch 195/200\n",
      "65/64 [==============================] - 46s 710ms/step - loss: 0.4048 - acc: 0.8999 - val_loss: 0.9707 - val_acc: 0.7553\n",
      "Learning rate:  5e-07\n",
      "Epoch 196/200\n",
      "65/64 [==============================] - 46s 709ms/step - loss: 0.4057 - acc: 0.8994 - val_loss: 0.9726 - val_acc: 0.7573\n",
      "Learning rate:  5e-07\n",
      "Epoch 197/200\n",
      "65/64 [==============================] - 46s 708ms/step - loss: 0.4085 - acc: 0.8950 - val_loss: 0.9738 - val_acc: 0.7573\n",
      "Learning rate:  5e-07\n",
      "Epoch 198/200\n",
      "65/64 [==============================] - 46s 704ms/step - loss: 0.4164 - acc: 0.8926 - val_loss: 0.9683 - val_acc: 0.7592\n",
      "Learning rate:  5e-07\n",
      "Epoch 199/200\n",
      "65/64 [==============================] - 46s 707ms/step - loss: 0.4112 - acc: 0.8974 - val_loss: 0.9694 - val_acc: 0.7573\n",
      "Learning rate:  5e-07\n",
      "Epoch 200/200\n",
      "65/64 [==============================] - 46s 715ms/step - loss: 0.4388 - acc: 0.8897 - val_loss: 0.9649 - val_acc: 0.7602\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff013962be0>"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "                            # Fit the model\n",
    "datagen = ImageDataGenerator(width_shift_range=0.1,\n",
    "                             height_shift_range=0.1,\n",
    "                             rotation_range=20,\n",
    "                             horizontal_flip=True,\n",
    "                             vertical_flip=False)\n",
    "\n",
    "model.fit_generator(datagen.flow(trDat, trLbl, batch_size=32),\n",
    "                    validation_data=(tsDat, tsLbl),\n",
    "                    epochs=200, #originally 200\n",
    "                    verbose=1,\n",
    "                    steps_per_epoch=len(trDat)/32,\n",
    "                    callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "elcNHYQM2P4z"
   },
   "outputs": [],
   "source": [
    "                            # Now the training is complete, we get\n",
    "                            # another object to load the weights\n",
    "                            # compile it, so that we can do \n",
    "                            # final evaluation on it\n",
    "modelGo.load_weights(filepath)\n",
    "modelGo.compile(loss='categorical_crossentropy', \n",
    "                optimizer=optmz, \n",
    "                metrics=['accuracy'])\n",
    "\n",
    "                            # Make classification on the test dataset\n",
    "predicts    = modelGo.predict(tsDat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "tspPEPMF2dCm",
    "outputId": "0a343dd5-5f66-4d39-861c-1ced0f63cc40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy (on testing dataset): 77.95%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         cat     0.8171    0.7424    0.7779       361\n",
      "        bird     0.8085    0.8398    0.8238       362\n",
      "         dog     0.7091    0.7524    0.7301       311\n",
      "\n",
      "    accuracy                         0.7795      1034\n",
      "   macro avg     0.7782    0.7782    0.7773      1034\n",
      "weighted avg     0.7816    0.7795    0.7796      1034\n",
      "\n",
      "[[268  38  55]\n",
      " [ 17 304  41]\n",
      " [ 43  34 234]]\n"
     ]
    }
   ],
   "source": [
    "                            # Prepare the classification output\n",
    "                            # for the classification report\n",
    "predout     = np.argmax(predicts,axis=1)\n",
    "testout     = np.argmax(tsLbl,axis=1)\n",
    "labelname   = ['cat',\n",
    "               'bird',\n",
    "               'dog']\n",
    "                                            # the labels for the classfication report\n",
    "\n",
    "testScores  = metrics.accuracy_score(testout,predout)\n",
    "confusion   = metrics.confusion_matrix(testout,predout)\n",
    "\n",
    "print(\"Best accuracy (on testing dataset): %.2f%%\" % (testScores*100))\n",
    "print(metrics.classification_report(testout,predout,target_names=labelname,digits=4))\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 335
    },
    "colab_type": "code",
    "id": "yFuEG_s42i1U",
    "outputId": "2df0e289-c5e8-479c-ec7a-7c38329a429a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/matplotlib/font_manager.py:1241: UserWarning: findfont: Font family ['Arial'] not found. Falling back to DejaVu Sans.\n",
      "  (prop.get_family(), self.defaultFamily[fontext]))\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "records     = pd.read_csv('/content/drive/My Drive/Colab/' + modelname +'.csv')\n",
    "plt.figure()\n",
    "plt.subplot(211)\n",
    "plt.plot(records['val_loss'])\n",
    "plt.plot(records['loss'])\n",
    "plt.yticks([0,0.20,0.40,0.60,0.80,1.00])\n",
    "plt.title('Loss value',fontsize=12)\n",
    "\n",
    "ax          = plt.gca()\n",
    "ax.set_xticklabels([])\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.plot(records['val_acc'])\n",
    "plt.plot(records['acc'])\n",
    "plt.yticks([0.6,0.7,0.8,0.9,1.0])\n",
    "plt.title('Accuracy',fontsize=12)\n",
    "plt.show()\n",
    "\n",
    "#from tensorflow.keras.utils import plot_model\n",
    "\n",
    "#plot_model(model, \n",
    "           #to_file=modelname+'_model.pdf', \n",
    "           #show_shapes=True, \n",
    "           #show_layer_names=False,\n",
    "           #rankdir='TB')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "PSUPR_CA2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
